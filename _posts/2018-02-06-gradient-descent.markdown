---
layout: post
title:  "[Machine Learning] Gradient Descent (Week 2)"
subtitle: ""
date:   2018-02-06 09:52:54 +0900
categories: machinelearning
background: '/img/posts/machine-learning.png'
author: cyc1am3n
comments: true
---
\* 이 포스트는 Coursera에 있는 Andrew Ng 교수님의 강의 [Machine Learning(링크)](https://www.coursera.org/learn/machine-learning)를 바탕으로 작성되었습니다.

## Gradient Descent  

저번 포스팅에서 언급했던 Linear Regression의 Cost Function의 최소값을 다시 한 번 생각해보자.  

다음은 θ<sub>0</sub>을 x축, θ<sub>1</sub>을 y축, J(θ<sub>0</sub>, θ<sub>1</sub>)을 z축으로 하는 그래프의 예이다.

{: refdef: style="text-align: center;"}  
![그림1](/img/posts/gradient-descent/gradient-descent-1.png)
{: refdef}
