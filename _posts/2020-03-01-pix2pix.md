---
layout: post
title:  "[Paper] Pix2Pix Review"
subtitle: "Image-to-Image Translation with Conditional Adversarial Networks"
post_description: "대표적인 GAN 논문인 pix2pix을 리뷰한 포스트입니다."
date:   2020-03-01 22:20:00 +0900
tags: [paper]
background: '/img/posts/pix2pix.png'
author: cyc1am3n
comments: true
---

> **제목**: **Image-to-Image Translation with Conditional Adversarial Networks**
>
> **저자**: *Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros* @ Berkeley AI Research (BAIR) Laboratory, UC Berkeley
>
> **학회/년도**: CVPR 2017

---

## Abstract

- input ↔ output image의 mapping과 이 **mapping을 학습시키기 위한 loss-function**을 배우는 네트워크

---

## Introduction

- image-to-image translation task를 정의
- CNN을 학습시키기에 효과적인 loss function을 디자인 하는 것에는 많은 수고가 필요하다.
  - ex) 단순하게 GT와 생성된 image 간의 Euclidean distance로 loss를 잡으면 blurry 한 결과가 나온다.
- 그 대신에 "실제와 구별하기 힘든 output을 만들자"라는 high-level goal을 가지고 GAN에 적용해본다면?
  - 위 예시처럼 blurry image는 fake라고 인식해서 이런 이미지는 안 만들어질 것이다.
  - GAN은 data에 맞춰진 loss를 통해 학습하고, 원래 같았으면 loss를 각각 만들어야했을 여러 task에서 사용할 수 있다.
- 이 논문에서는 conditional GAN을 활용해서 image-to-image translation의 다양한 문제를 해결하는 모델을 제시할 것이다. (게다가 간단하기까지함)
- 코드는 [여기](https://github.com/phillipi/pix2pix)서 볼 수 있다.

---

## Related work

### Structured losses for image modeling

- unstructured loss와 structured loss의 차이는?
- conditional GAN 은 structured loss를 학습하고 output과 target 간의 다른 구조에 페널티를 준다.

### Conditional GANs

- GAN을 conditional setting에서 사용한 것은 이 논문이 처음이 아니며 다양한 분야(future frame prediction, image generation, 심지어 image-to-image mapping 까지도)에서 시도되었다.

- 이 논문이 제시하는 방법은 한정된 application에서만 쓸 수 있는 것이 아니며 다른 것들 보다 간단하다.
- 이전 방법과 generator와 discriminator의 네트워크 아키텍쳐 부분에서 다르다. (다시 살펴볼 예정)
  - generator로 **U-Net** 기반 아키텍쳐를 골라서 원본 이미지의 정보 손실을 최소화시켰다.
  - discriminator로 **Patch GAN** 이라는 아키텍쳐를 골라서 디테일한 feature까지 잡아낼 수 있다.

---

## Method

{% include image.html file="https://user-images.githubusercontent.com/11629647/75626368-70caf900-5c0a-11ea-9a4a-a0a5057cb802.png" class="center-75"%} 

-  (vanilla) GAN은 random noise $$z$$ 에서 output image $$y$$를 만들어낸다. $$G:z\rightarrow y$$
-  conditional GAN은 source image $$x$$와 $$z$$ 에서 $$y$$ 를 만들어낸다. $$G:\{x,z\}\rightarrow y$$
   - 이때 discriminator에 넣을 input에도 $$x$$를 같이 넣어준다. (Figure 2 참고, 그리고 사실 $$z$$ 안씀)

<br/>

### Objective

- conditional GAN의 loss 는 아래 식과 같이 표현할 수 있다.

$$
\mathcal{L}_{cGAN}(G, D)=\mathbb{E}_{x,y}[\log D(x, y)]+\mathbb{E}_{x,z}[\log(1-D(x,G(x,z)))] \tag{1}
$$

- discriminator 에 condition을 주지 않으면 어떻게 되는지 비교하기 위해서 다음 loss를 가지고도 실험할 것이다.

$$
\mathcal{L}_{GAN}(G, D)=\mathbb{E}_{y}[\log D(y)]+\mathbb{E}_{x,z}[\log(1-D(G(x,z)))] \tag{2}
$$

- 또한 고전적인 접근법을 참고해서 generator가 생성한 image와 target image의 차이를 줄이는 GAN loss를 추가하면 성능이 좋아진다.

  - 이때 L2보다 L1을 사용하면 덜 blur한 이미지가 만들어진다.
    $$
    \mathcal{L}_{L1}(G)=\mathbb{E}_{x,y,z}[||y-G(x,z)||_1]\tag{3}
    $$

- 이를 다 합쳐서 objective를 만들면 다음과 같다.

$$
G^*=\text{arg }\underset{G}{\text{min}}\ \underset{D}{\text{max}}\ \mathcal{L}_{cGAN}(G,D)+\lambda \mathcal{L}_{L1}(G)\tag{4}
$$

- generator에 가우시안 noise $$z$$ 를 같이 입력해봤는데, 그다지 효과적이지는 않았다.(오히려 discriminator가 noise를 무시함)
  - 그래서 이 논문의 모델에서는 generator의 입력에 noise $$z$$​를 넣지 않고, 학습 및 평가시에 드롭 아웃 형태의 노이즈를 넣어주는 식으로 전환함.
  - 하지만 stochasticity가 떨어지는 모습을 발견했고, 이 문제를 해결하는 conditional GAN을 설계하는 것은 future work으로..

<br/>

### Network architectures

- generator와 discriminator에는 Conv-BatchNorm-ReLu 로 구성된 모듈들을 사용했다.

#### Generator with skips

- image-to-image translation의 어려운 점은 고해상도 입력-출력 매핑이라는 점이다.
  - 그래서 기존의 encoder-decoder 네트워크를 사용한다면 encoder 통과시 downsample 되면서 정보의 손실이 발생할 수 있게 된다.
- 이를 보완해 low-level information을 input-output가 skip connection을 통해 공유하는 형태인 U-Net 을 사용하자.
  - 총 $$n$$개의 layer가 있다고 한다면, $$i$$ 번째 layer와 $$n-i$$ 번째 layer를 연결한다.
  - skip connection은 간단히 channel을 concatenate하는 식으로 만들었다.

{% include image.html file="https://user-images.githubusercontent.com/11629647/75626371-74f71680-5c0a-11ea-8501-9f9a5d13d7fb.png" class="center-75"%} 

#### Markovian discriminator (PatchGAN)

- L1이나 L2 loss를 쓰면 blur하게 high frequency 정보를 잘 잡아낼 수는 없지만 low frequency 정보는 잘 잡아낸다.
- 이 점을 적용해 GAN discriminator 가 high frequency를 잡아내고 L1이 low frequency를 잡아내도록 하자. (식 4 참고)
- discriminator가 high frequency를 잡도록 방향을 잡아 이미지의 일부분에 집중해서 local patch를 보고 구분하도록 하는 PatchGAN을 설계했다.
  - 이미지를 $$N\times N$$ 개의 patch로 나눠 각 patch가 real인지 fake 구분하도록 만들었다.
  - $$N^2$$ 개의 output을 averaging해서 최종 output으로 사용했다.
- patch를 많이 만들면 파라미터도 적게 들고 빠르고 다양한 크기의 이미지에도 적용할 수 있다. (퀄리티도 좋음)
- PatchGAN discriminator는 패치 크기 이상으로 분리된 픽셀 사이의 독립성을 가정해 이미지를 Markov random field로 모델링한다.
  - 이를 통해 PatchGAN이 texture/style 을 이해할 수 있게 되는 것이다.

<br/>

### Optimization and inference

- Adam
  - lr = 0.0002, $$\beta_1$$ = 0.5, $$\beta_2$$ = 0.999
- test 시에도 dropout을 적용하며, batch norm을 쓸 때도 test data에서 얻은 통계를 사용한다.
- batch가 1인 경우에는 instance normalization이 되며 이는 image generation task 에서 효과적인 결과를 낸다.
  - batch size를 1 ~ 10 까지 두고 실험을 해보았다.

---

## Experiment

### Analysis of the objective function

{% include image.html file="https://user-images.githubusercontent.com/11629647/75626421-cbfceb80-5c0a-11ea-849b-39be21f53ef8.png" class="center-95"%} 

- 위 그림은 식 $$(4)$$ 에서 각 loss의 component를 가지고  label → photo 문제에 대한 결과를 낸 것이다.

- 여기에 대해 FCN score를 내보면 다음과 같다.

  {% include image.html file="https://user-images.githubusercontent.com/11629647/75626428-e0d97f00-5c0a-11ea-87ee-2f5303038aeb.png" class="center-75"%} 

- L1 loss는 edge의 위치를 정확하게 특정하기 힘들어 blur 한 이미지를 만들기 때문에 average인 grayish color가 나오게 된다.

- 반면에 adversarial loss(GAN loss)에서는 grayish color는 realistic 하지 않다고 판단하기에 좀더 true color distribution에 가까운 이미지를 만들어내도록 한다. (그래서 cGAN loss만 쓰는게 더 colorful 해 보임)

### Analysis of the generator architecture

{% include image.html file="https://user-images.githubusercontent.com/11629647/75626430-e59e3300-5c0a-11ea-9c10-bbbb2a90eca1.png" class="center-75"%} 

{% include image.html file="https://user-images.githubusercontent.com/11629647/75626437-ec2caa80-5c0a-11ea-9162-427cfc3a6cd2.png" class="center-75"%} 

- generator의 아키텍쳐로 Encoder-Decoder를 쓰기에는 적합하지 않다.(오히려 cGAN loss를 쓰면 떨어짐)
- U-Net을 써야 성능이 오른다.

### From PixelGANs to PatchGANs to ImageGANs

- 다양한 크기의 patch를 discriminator 에 넣어 실험을 진행해보았다.
  - $$1\times 1$$ 이 PixelGAN이고, 이미지 원본 크기인 $$286\times286$$를 ImageGAN이라고 했다.

{% include image.html file="https://user-images.githubusercontent.com/11629647/75626459-15e5d180-5c0b-11ea-863d-4659292eda8b.png" class="center-95"%} 

{% include image.html file="https://user-images.githubusercontent.com/11629647/75626461-18e0c200-5c0b-11ea-9f44-23d68fbf4f79.png" class="center-75"%} 

- PixelGAN이나 ImageGAN보다는 PatchGAN의 성능이 잘나오더라.
  - PixelGAN은 spatial 정보를 잡을 수 없고 ImageGAN은 파라미터와 depth가 더 많이 필요해 학습시키기가 어렵다.

### Semantic Segmentation

- Semantic Segmentation은 output이 input보다 덜 복잡한 task이다.

{% include image.html file="https://user-images.githubusercontent.com/11629647/75626463-1d0cdf80-5c0b-11ea-98c9-c4520714af67.png" class="center-95"%} 

{% include image.html file="https://user-images.githubusercontent.com/11629647/75626464-2007d000-5c0b-11ea-8e7d-8a985fe0ba9d.png" class="center-75"%} 

- 그래서 그런지 cGAN loss를 써도 잘 하긴 하지만 L1 loss만 있어도 잘 하더라.